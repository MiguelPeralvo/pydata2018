{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nMetrics to judge the sucess of a model\n=======================================\n\nPro & cons of various performance metrics.\n\nThe simple way to use a scoring metric during cross-validation is\nvia the `scoring` parameter of\n:func:`sklearn.model_selection.cross_val_score`.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Regression settings\n-----------------------\n\nThe Boston housing data\n........................\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn import datasets\nboston = datasets.load_boston()\n\n# Shuffle the data\nfrom sklearn.utils import shuffle\ndata, target = shuffle(boston.data, boston.target, random_state=0)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "A quick plot of how each feature is related to the target\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from matplotlib import pyplot as plt\n\nfor feature, name in zip(data.T, boston.feature_names):\n    plt.figure(figsize=(4, 3))\n    plt.scatter(feature, target)\n    plt.xlabel(name, size=22)\n    plt.ylabel('Price (US$)', size=22)\n    plt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We will be using a random forest regressor to predict the price\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Explained variance vs Mean Square Error\n.......................................\n\nThe default score is explained variance\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.model_selection import cross_val_score\nprint(cross_val_score(regressor, data, target))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Explained variance is convienent because it has a natural scaling: 1 is\nperfect prediction, and 0 is around chance\n\nNow let us see which houses are easier to predict:\n\nNot along the Charles river (feature 3)\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(regressor, data[data[:, 3] == 0],\n                      target[data[:, 3] == 0]))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Along the Charles river\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(regressor, data[data[:, 3] == 1],\n                      target[data[:, 3] == 1]))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "So the houses along the Charles are harder to predict?\n\nIt's not so easy to conclude this from the explained variance: in two\ndifferent sets of observations, the variance of the target differs, and\nthe explained variance is a relative measure\n\n**MSE**: We can use the mean squared error (here negated)\n\nNot along the Charles river\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(regressor, data[data[:, 3] == 0],\n                      target[data[:, 3] == 0],\n                      scoring='neg_mean_squared_error'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Along the Charles river\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(regressor, data[data[:, 3] == 1],\n                      target[data[:, 3] == 1],\n                      scoring='neg_mean_squared_error'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "So the error is larger along the Charles river\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Mean Squared Error versus Mean Absolute Error\n..................................................\n\nWhat if we want to report an error in dollars, meaningful for an\napplication?\n\nThe Mean Absolute Error is useful for this goal\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(regressor, data, target,\n                      scoring='neg_mean_absolute_error'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Summary\n.........\n\n* **explained variance**: scaled with regards to chance: 1 = perfect,\n  0 = around chance, but it shouldn't used to compare predictions\n  across datasets\n\n* **mean absolute error**: enables comparison across datasets in the\n  units of the target\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Classification settings\n-----------------------\n\nThe digits data\n.................\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "digits = datasets.load_digits()\n# Let us try to detect sevens:\nsevens = (digits.target == 7)\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Accuracy and its shortcomings\n.............................\n\nThe default metric is the accuracy: the averaged fraction of success.\nIt takes values between 0 and 1, where 1 is perfect prediction\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(classifier, digits.data, sevens))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "However, a stupid classifier can each good prediction wit imbalanced\nclasses\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.dummy import DummyClassifier\nmost_frequent = DummyClassifier(strategy='most_frequent')\nprint(cross_val_score(most_frequent, digits.data, sevens))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Balanced accuracy (available in development scikit-learn versions)\nfixes this, but can have surprising behaviors, such as being negative\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Precision, recall, and their shortcomings\n..........................................\n\nWe can measure separately false detection and misses\n\n**Precision**: Precision counts the ratio of detections that are\ncorrect\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(classifier, digits.data, sevens,\n                      scoring='precision'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Our classifier has a good precision: most of the sevens that it\npredicts are really sevens.\n\nAs predicting the most frequent never predicts sevens, precision is ill\ndefined. Scikit-learn puts it to zero\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(most_frequent, digits.data, sevens,\n                      scoring='precision'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "**Recall**: Recall counts the fraction of class 1 actually detected\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(classifier, digits.data, sevens, scoring='recall'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Our recall isn't as good: we miss many sevens\n\nBut predicting the most frequent never predicts sevens:\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(most_frequent, digits.data, sevens, scoring='recall'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "**Note**: Measuring only the precision without the recall makes no\nsense, it is easy to maximize one at the cost of the other. Ideally,\nclassifiers should be compared on a precision at a given recall\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Area under the ROC curve\n..........................\n\nIf the classifier provides a decision function that can be thresholded\nto control false positives versus false negatives, the ROC curve\nsummarise the different tradeoffs that can be achieved by varying this\nthreshold.\n\nIts Area Under the Curve (AUC) is a useful metric where 1 is perfect\nprediction and .5 is chance, independently of class imbalance\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(classifier, digits.data, sevens, scoring='roc_auc'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(most_frequent, digits.data, sevens, scoring='roc_auc'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Average precision\n..................\n\nWhen the classifier exposes its unthresholded decision, another\ninteresting metric is the average precision for all recall. Compared to\nROC AUC it has a more linear behavior for very rare classes. Indeed,\nwith very rare classes, small changes in the ROC AUC may mean large\nchanges in terms of precision\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(classifier, digits.data, sevens,\n                      scoring='average_precision'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Naive decisions are no longer at .5\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(most_frequent, digits.data, sevens,\n                      scoring='average_precision'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Multiclass and multilabel settings\n...................................\n\nTo simplify the discussion, we have reduced the problem to detecting\nsevens, but maybe it is more interesting to predict the digit: a\n10-class classification problem\n\n**Accuracy** The accuracy is naturally defined in such multiclass settings\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(classifier, digits.data, digits.target))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The most frequent label is no longer a very interesting baseline\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "random_choice = DummyClassifier()\nprint(cross_val_score(random_choice, digits.data, digits.target))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Precision and recall need the notion of specific class to detect\n(called positive class) and are not that easily defined in these\nsettings, hence ROC AUC cannot be easily computed.\n\nThese notions are however well defined in a multi-label problem.\nIn such a problem, the goal is to assign one or more labels to each\ninstance, as opposed to a multiclass. A multiclass problem can be\nturned into a multilabel one, though the prediction will then be\nslightly different\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\ndigit_labels = LabelBinarizer().fit_transform(digits.target)\nprint(digit_labels[:15])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The ROC AUC can then be computed for each label\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(classifier, digits.data, digit_labels,\n                      scoring='roc_auc'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "as well as the average precision\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(classifier, digits.data, digit_labels,\n                      scoring='average_precision'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Note that the confusion between classes may not well be captured in\nSuch a measure, as in multiclass predictions are exclusive, and not\nin multilabel.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Summary\n..........\n\nClass imbalance and the tradeoffs between accepting many misses or many\nfalse detections are the things to keep in mind in classification.\n\nIn single-class settings, ROC AUC and average precision give nice\nsummaries to compare classifiers when the threshold can be varied. In\nmulticlass settings, this is harder, unless we are willing to consider\nthe problem as multiple single-class problems (one-vs-all).\n\n____________________\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.14", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}