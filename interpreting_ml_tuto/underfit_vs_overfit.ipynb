{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nUnderfit vs overfit: do I need more data, or more complex models?\n==================================================================\n\nThis is adapted from the scikit-learn chapter in the scipy lectures\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "A toy problem: fitting polynomes\n---------------------------------\n\nData generation model\n......................\n\nWe consider data generated by the following mechanism, as a toy model\nof housing prices\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import numpy as np\n\ndef generating_func(x, err=1):\n    return np.random.normal(10 - 1. / (x + 0.1), err)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Polynomial regression\n......................\n\nThe crucial hyperparameter is the degree\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nmodel = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Train error versus test error\n--------------------------------\n\nGenerate some data with few samples, for easy understanding\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "n_samples = 8\nnp.random.seed(0)\nx = 10 ** np.linspace(-2, 0, n_samples)\ny = generating_func(x)\n\n# For plotting\nx_plot = np.linspace(-0.2, 1.2, 1000)\n\n# randomly sample the data\nnp.random.seed(1)\nx_test = np.random.random(200)\ny_test = generating_func(x_test)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Underfit: high bias\n...................\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import matplotlib.pyplot as plt\nplt.figure(figsize=(6, 4))\nplt.scatter(x, y, marker='x', c='k', s=100)\nplt.scatter(x_test, y_test, marker='.', c='k', s=50, alpha=.2)\n\ndegree = 1\nmodel = make_pipeline(PolynomialFeatures(degree), LinearRegression())\nmodel.fit(x[:, np.newaxis], y)\nplt.plot(x_plot, model.predict(x_plot[:, np.newaxis]), '-b')\n\nplt.xlim(-0.2, 1.2)\nplt.ylim(0, 12)\nplt.xlabel('house size')\nplt.ylabel('price')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "This model has a low train score (high train error):\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(model.score(x[:, np.newaxis], y))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "If we apply it to the unseen data, the test error is\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(model.score(x_test[:, np.newaxis], y_test))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Overfit: high variance\n.......................\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "plt.figure(figsize=(6, 4))\nplt.scatter(x, y, marker='x', c='k', s=100)\nplt.scatter(x_test, y_test, marker='.', c='k', s=50, alpha=.2)\n\ndegree = 6\nmodel = make_pipeline(PolynomialFeatures(degree), LinearRegression())\nmodel.fit(x[:, np.newaxis], y)\nplt.plot(x_plot, model.predict(x_plot[:, np.newaxis]), '-b')\n\nplt.xlim(-0.2, 1.2)\nplt.ylim(0, 12)\nplt.xlabel('house size')\nplt.ylabel('price')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "This model has a high train score (very low train error):\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(model.score(x[:, np.newaxis], y))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "If we apply it to the unseen data, the test error is\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(model.score(x_test[:, np.newaxis], y_test))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Learning curves\n--------------------------------\n\nFit polynomes of different degrees to a dataset: for too small a\ndegree, the model *underfits*, while for too large a degree, it\noverfits.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Generate a larger dataset\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "np.random.seed(1)\nx = np.random.random(200)\ny = generating_func(x)\n\n# split into training, validation, and testing sets.\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.4)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Show the training and validation sets\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "plt.figure(figsize=(6, 4))\nplt.scatter(x_train, y_train, color='red', label='Training set')\nplt.scatter(x_test, y_test, color='blue', label='Test set')\nplt.title('The data')\nplt.legend(loc='best')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Validation curve varying model complexity\n------------------------------------------\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.model_selection import validation_curve\n\ndegrees = np.arange(1, 21)\nmodel = make_pipeline(PolynomialFeatures(), LinearRegression())\n\n# The parameter to vary is the \"degrees\" on the pipeline step\n# \"polynomialfeatures\"\ntrain_scores, validation_scores = validation_curve(\n                 model, x[:, np.newaxis], y,\n                 param_name='polynomialfeatures__degree',\n                 param_range=degrees)\n\n# Plot the mean train error and validation error across folds\nplt.figure(figsize=(6, 4))\nplt.plot(degrees, validation_scores.mean(axis=1), lw=2,\n         label='cross-validation')\nplt.plot(degrees, train_scores.mean(axis=1), lw=2, label='training')\n\nplt.legend(loc='best')\nplt.xlabel('degree of fit')\nplt.ylabel('explained variance')\nplt.title('Validation curve')\nplt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Learning curves: varying the amount of data\n--------------------------------------------\n\nPlot train and test error with an increasing number of samples\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# A learning curve for d=1, 5, 15\nfor d in [1, 5, 15]:\n    model = make_pipeline(PolynomialFeatures(degree=d), LinearRegression())\n\n    from sklearn.model_selection import learning_curve\n    train_sizes, train_scores, validation_scores = learning_curve(\n        model, x[:, np.newaxis], y,\n        train_sizes=np.logspace(-1, 0, 20))\n\n    # Plot the mean train error and validation error across folds\n    plt.figure(figsize=(6, 4))\n    plt.plot(train_sizes, validation_scores.mean(axis=1),\n            lw=2, label='cross-validation')\n    plt.plot(train_sizes, train_scores.mean(axis=1),\n                lw=2, label='training')\n    plt.ylim(ymin=-.1, ymax=1)\n\n    plt.legend(loc='best')\n    plt.xlabel('number of train samples')\n    plt.ylabel('explained variance')\n    plt.title('Learning curve (degree=%i)' % d)\n    plt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Summary\n--------\n\nComparing train and test error, ideal when vary parameters and amount\nof data, tells us whether the model is:\n\n* **Overfitting**, ie data limited, and should not be made more complex\n  and regularization is important, or ideally aquiring more data\n\n* **Underfitting**, ie not rich enough for the data available, and\n  should be made complex\n\n\n_____________________________________\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.14", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}